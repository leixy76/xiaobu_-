<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>《机器学习的数学》核心摘要</title>
    <style>
        :root {
            --bg-color: #f8f9fa;
            --text-color: #212529;
            --header-color: #007bff;
            --keyword-color: #0056b3;
            --summary-bg: #ffffff;
            --border-color: #dee2e6;
            --link-color: #007bff;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            background-color: var(--bg-color);
            color: var(--text-color);
            line-height: 1.7;
            margin: 0;
            padding: 20px;
        }
        main {
            max-width: 900px;
            margin: 0 auto;
            background-color: var(--summary-bg);
            border-radius: 10px;
            padding: 2em;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            border: 1px solid var(--border-color);
        }
        h1 {
            color: var(--header-color);
            text-align: center;
            border-bottom: 2px solid var(--header-color);
            padding-bottom: 10px;
            margin-bottom: 1.5em;
        }
        details {
            margin-bottom: 1em;
            border-left: 3px solid var(--border-color);
            padding-left: 1.5em;
            border-radius: 5px;
            background-color: #fdfdfd;
            transition: all 0.3s ease-in-out;
        }
        details[open] {
            background-color: #f9f9f9;
            border-left-color: var(--header-color);
        }
        summary {
            font-size: 1.2em;
            font-weight: bold;
            cursor: pointer;
            padding: 0.8em;
            color: var(--header-color);
            outline: none;
        }
        summary::-webkit-details-marker {
            color: var(--header-color);
        }
        ul {
            list-style-type: ' ➤ ';
            padding-left: 25px;
        }
        li {
            margin-bottom: 0.8em;
        }
        .keyword {
            color: var(--keyword-color);
            font-weight: bold;
            background-color: rgba(0, 123, 255, 0.1);
            padding: 2px 5px;
            border-radius: 4px;
        }
        .excerpt {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 1em;
            margin-top: 1em;
            font-style: italic;
            color: #6c757d;
        }
        .excerpt summary {
            font-size: 1em;
            color: #6c757d;
            font-weight: normal;
        }
        .excerpt p {
            margin-top: 0.5em;
        }
        code {
            background-color: #e9ecef;
            color: #c7254e;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }
        .intro-text {
            font-size: 1.1em;
            text-align: center;
            margin-bottom: 2em;
            color: #495057;
        }
        @media (max-width: 768px) {
            main {
                padding: 1em;
            }
            summary {
                font-size: 1.1em;
            }
        }
    /* 主容器布局调整 - 移除flex布局以保持原始排版 */

.attachments-panel {
    width: 250px;
    background: #fff;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    height: fit-content;
    position: sticky;
    top: 20px;
    flex-shrink: 0;
    z-index: 1000;
    border: 1px solid #e0e0e0;
}
.attachments-panel h3 {
    color: #004085;
    margin-top: 0;
    margin-bottom: 15px;
    text-align: center;
    border-bottom: 2px solid #004085;
    padding-bottom: 10px;
}
.attachment-item {
    background-color: #f8f9fa;
    border: 1px solid #ced4da;
    border-radius: 5px;
    padding: 15px;
    margin-bottom: 15px;
    transition: all 0.3s ease;
}
.attachment-item:hover {
    background-color: #e9ecef;
    border-color: #004085;
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.attachment-item h4 {
    margin: 0 0 8px 0;
    color: #004085;
    font-size: 14px;
}
.attachment-item p {
    margin: 0 0 10px 0;
    font-size: 12px;
    color: #6c757d;
    line-height: 1.4;
}
.download-btn {
    display: inline-block;
    background-color: #28a745;
    color: white;
    text-decoration: none;
    padding: 8px 16px;
    border-radius: 4px;
    font-size: 12px;
    font-weight: bold;
    transition: background-color 0.3s ease;
    width: 100%;
    text-align: center;
    box-sizing: border-box;
}
.download-btn:hover {
    background-color: #218838;
    text-decoration: none;
    color: white;
}
.download-btn:before {
    content: "▼ ";
    margin-right: 5px;
}

/* 可折叠浮动框样式 */
.attachments-panel {
    position: fixed;
    top: 10px;
    right: 10px;
    width: auto; /* 自适应宽度 */
    min-width: 250px; /* 最小宽度 */
    max-width: calc(100vw - 60px); /* 最大宽度，留出边距 */
    max-height: calc(100vh - 80px); /* 留出更多空间避免滚动条 */
    overflow-y: auto;
    overflow-x: hidden; /* 隐藏横向滚动条 */
    z-index: 9999;
    box-shadow: 0 8px 24px rgba(0,0,0,0.25);
    border: 2px solid #004085;
    transform: translateX(100%); /* 默认完全隐藏 */
    transition: transform 0.3s ease;
    /* 自定义滚动条样式 */
    scrollbar-width: thin;
    scrollbar-color: #004085 #f0f0f0;
}

/* WebKit浏览器滚动条样式 */
.attachments-panel::-webkit-scrollbar {
    width: 6px;
}

.attachments-panel::-webkit-scrollbar-track {
    background: #f0f0f0;
    border-radius: 3px;
}

.attachments-panel::-webkit-scrollbar-thumb {
    background: #004085;
    border-radius: 3px;
}

.attachments-panel::-webkit-scrollbar-thumb:hover {
    background: #0056b3;
}

/* 展开时显示 */
.attachments-panel.expanded {
    transform: translateX(0);
}

/* 移除container的margin-right设置，保持原始布局 */

.attachment-item {
    width: 250px;
    margin-right: 0;
    display: block;
    margin-bottom: 12px;
    white-space: nowrap; /* 防止文本换行导致宽度过大 */
}

.attachment-item h4 {
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis; /* 长文本显示省略号 */
    max-width: 100%;
}

.attachments-panel h3 {
    font-size: 16px;
    margin-bottom: 12px;
    white-space: nowrap; /* 标题不换行 */
}

.attachments-panel a {
    white-space: nowrap; /* 链接文本不换行 */
    overflow: hidden;
    text-overflow: ellipsis;
    display: inline-block;
    max-width: 100%;
}

/* 浮动切换按钮 */
.float-toggle {
    position: fixed;
    top: 10px;
    right: 10px;
    width: 50px;
    height: 50px;
    background: #004085;
    color: white;
    border: none;
    border-radius: 50%;
    font-size: 20px;
    cursor: pointer;
    z-index: 10000;
    box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    transition: all 0.3s ease;
    display: flex;
    align-items: center;
    justify-content: center;
}

.float-toggle:hover {
    background: #0056b3;
    transform: scale(1.1);
}

.float-toggle:active {
    transform: scale(0.95);
}

/* 当面板展开时，按钮位置动态调整 */
.attachments-panel.expanded ~ .float-toggle {
    right: 20px; /* 保持在面板左侧的固定位置 */
    opacity: 0.7; /* 展开时降低透明度，避免遮挡 */
}
    </style>
</head>
<body>
    <main>
        <h1>《机器学习的数学》(Mathematics for Machine Learning) 核心摘要</h1>
        <p class="intro-text">
            这份交互式摘要旨在以极低的认知负荷，提炼全书的核心要点。您可以点击各个标题来展开或折叠详细内容。
        </p>

        <details open>
            <summary>核心观点与目标读者</summary>
            <ul>
                <li><strong>核心目标:</strong> 本书旨在弥合机器学习实践者在<strong class="keyword">数学基础</strong>（特别是线性代数、微积分和概率论）与<strong class="keyword">机器学习算法</strong>理解之间的鸿沟。它不是一本传统的机器学习算法大全，而是一本专注于数学基础的“导航手册”。</li>
                <li><strong>教学方法:</strong> 全书分为两大部分：
                    <ol>
                        <li><strong>第一部分：数学基础。</strong>系统性地讲解机器学习所需的核心数学工具。</li>
                        <li><strong>第二部分：核心机器学习问题。</strong>将第一部分的数学概念应用于四个经典的机器学习任务（回归、降维、密度估计、分类），展示数学如何驱动算法设计。</li>
                    </ol>
                </li>
                <li><strong>目标读者 (三类人群):</strong>
                    <ul>
                        <li><strong class="keyword">敏锐的聆听者 (Astute Listener):</strong> 希望理解机器学习底层原理的用户、产品经理或决策者。</li>
                        <li><strong class="keyword">经验丰富的演奏家 (Experienced Artist):</strong> 能够熟练使用各种工具库，但希望深入理解其背后原理的数据科学家和工程师。</li>
                        <li><strong class="keyword">初露锋芒的作曲家 (Fledgling Composer):</strong> 需要创造新算法和模型的研究者与开发者。</li>
                    </ul>
                </li>
            </ul>
            <details class="excerpt">
                <summary>英文原文摘录 (Original Excerpt)</summary>
                <p>This book brings the mathematical foundations of basic machine learning concepts to the fore and collects the information in a single place so that this skills gap is narrowed or even closed.</p>
                <p>The book is split into two parts, where Part I lays the mathematical foundations and Part II applies the concepts from Part I to a set of fundamental machine learning problems, which form four pillars of machine learning...</p>
            </details>
        </details>

        <details>
            <summary>第一部分：数学基础 (Part I: Mathematical Foundations)</summary>
            
            <details>
                <summary>第2章: 线性代数 (Linear Algebra)</summary>
                <ul>
                    <li><strong class="keyword">核心地位:</strong> 线性代数是机器学习的语言，用于描述和操作数据。</li>
                    <li><strong class="keyword">向量 (Vectors):</strong> 将数据点（如一张图片、一个用户画像）表示为向量。</li>
                    <li><strong class="keyword">矩阵 (Matrices):</strong> 将整个数据集或模型参数表示为矩阵。</li>
                    <li><strong class="keyword">核心概念:</strong> 线性方程组、向量空间、线性无关、基与秩、线性映射。这些是理解后续模型（如线性回归、PCA）的基础。</li>
                </ul>
                <details class="excerpt">
                    <summary>英文原文摘录 (Original Excerpt)</summary>
                    <p>Linear algebra is the study of vectors and certain rules to manipulate vectors. ... In general, vectors are special objects that can be added together and multiplied by scalars to produce another object of the same kind. From an abstract mathematical viewpoint, any object that satisfies these two properties can be considered a vector.</p>
                </details>
            </details>

            <details>
                <summary>第3章: 解析几何 (Analytic Geometry)</summary>
                <ul>
                    <li><strong class="keyword">核心思想:</strong> 为向量空间引入几何直觉，使我们能够衡量向量间的关系。</li>
                    <li><strong class="keyword">内积 (Inner Product):</strong> <strong>核心工具</strong>，用于定义长度、距离和角度。它将代数操作与几何概念联系起来。</li>
                    <li><strong class="keyword">范数 (Norms):</strong> 定义向量的“长度”或“大小”。</li>
                    <li><strong class="keyword">正交性 (Orthogonality):</strong> 定义向量间的“垂直”关系，是构建高效基和理解投影的关键。</li>
                    <li><strong class="keyword">正交投影 (Orthogonal Projections):</strong> 将向量投影到低维子空间，是PCA和线性回归的几何基础。</li>
                </ul>
            </details>
            
            <details>
                <summary>第4章: 矩阵分解 (Matrix Decompositions)</summary>
                <ul>
                    <li><strong class="keyword">核心思想:</strong> 将复杂矩阵分解为更简单、更具解释性的矩阵的乘积，类似于数字的质因数分解。</li>
                    <li><strong class="keyword">特征值与特征向量 (Eigenvalues & Eigenvectors):</strong> 揭示了矩阵变换的“主方向”。特征向量在变换中只进行缩放，缩放因子即特征值。这是理解PCA的核心。</li>
                    <li><strong class="keyword">奇异值分解 (SVD):</strong> <strong>极其重要</strong>的分解方法，适用于任何矩阵（无论方阵与否）。它将矩阵分解为旋转、缩放和另一次旋转。是许多降维和推荐系统的基础。</li>
                    <li>其他分解: Cholesky分解（针对对称正定矩阵）、对角化。</li>
                </ul>
            </details>

            <details>
                <summary>第5章: 向量微积分 (Vector Calculus)</summary>
                <ul>
                    <li><strong class="keyword">核心作用:</strong> 提供了优化机器学习模型参数的数学工具。</li>
                    <li><strong class="keyword">梯度 (Gradients):</strong> 函数在某点上<strong class="keyword">增长最快</strong>的方向，是梯度下降算法的基础。</li>
                    <li><strong class="keyword">雅可比矩阵 (Jacobian) & 海森矩阵 (Hessian):</strong> 梯度对多维函数的一阶和二阶推广，分别描述局部线性和曲率信息。</li>
                    <li><strong class="keyword">链式法则 (Chain Rule):</strong> <strong>核心法则</strong>，用于计算复合函数的梯度。</li>
                    <li><strong class="keyword">反向传播 (Backpropagation):</strong> 链式法则在神经网络中的高效实现，是深度学习训练的核心算法。</li>
                </ul>
            </details>

            <details>
                <summary>第6章: 概率与分布 (Probability and Distributions)</summary>
                <ul>
                    <li><strong class="keyword">核心作用:</strong> 为机器学习中的<strong class="keyword">不确定性</strong>建模。</li>
                    <li><strong class="keyword">基本法则:</strong> 加法法则（边缘化）和乘法法则（条件概率）。</li>
                    <li><strong class="keyword">贝叶斯定理 (Bayes' Theorem):</strong> <strong>核心定理</strong>，用于在观察到新数据后更新我们的信念（从先验到后验）。<code>p(θ|D) ∝ p(D|θ)p(θ)</code></li>
                    <li><strong class="keyword">重要分布:</strong>
                        <ul>
                            <li><strong>高斯分布 (Gaussian):</strong> 因其分析上的便利性和中心极限定理而无处不在。</li>
                            <li><strong>伯努利/二项分布 (Bernoulli/Binomial):</strong> 用于建模二元或计数数据。</li>
                            <li><strong>指数族 (Exponential Family):</strong> 一个统一了许多常见分布的框架，具有良好的计算特性（如共轭性）。</li>
                        </ul>
                    </li>
                </ul>
            </details>

            <details>
                <summary>第7章: 连续优化 (Continuous Optimization)</summary>
                <ul>
                    <li><strong class="keyword">核心任务:</strong> “学习”过程在数学上通常被表述为找到使目标函数（如损失函数）最小化的参数。</li>
                    <li><strong class="keyword">梯度下降 (Gradient Descent):</strong> <strong>最基础的优化算法</strong>。沿着梯度的反方向迭代更新参数以寻找局部最小值。随机梯度下降（SGD）是其在大数据集上的高效变体。</li>
                    <li><strong class="keyword">约束优化与拉格朗日乘子 (Constrained Optimization & Lagrange Multipliers):</strong> 一种将约束问题转化为无约束问题的技巧，是推导SVM对偶形式的关键。</li>
                    <li><strong class="keyword">凸优化 (Convex Optimization):</strong> 一类特殊的优化问题，其<strong class="keyword">局部最优解就是全局最优解</strong>。许多机器学习问题被设计成凸问题以保证能找到最优解。</li>
                </ul>
            </details>
        </details>

        <details>
            <summary>第二部分：核心机器学习问题 (Part II: Central Machine Learning Problems)</summary>

            <details>
                <summary>第9章: 线性回归 (Linear Regression)</summary>
                <ul>
                    <li><strong>问题:</strong> 找到一个线性函数，将输入特征映射到一个连续的输出值。</li>
                    <li><strong>数学工具:</strong>
                        <ul>
                            <li><strong class="keyword">线性代数:</strong> 将问题表述为求解线性方程组 <code>y = Xθ</code>。最小二乘解为 <code>θ = (XᵀX)⁻¹Xᵀy</code>。</li>
                            <li><strong class="keyword">解析几何:</strong> 最小二乘解可以被看作是将观测向量 <code>y</code> <strong class="keyword">正交投影</strong>到由特征矩阵 <code>X</code> 的列所张成的子空间上。</li>
                            <li><strong class="keyword">概率论:</strong> 假设观测噪声服从高斯分布，最大似然估计（MLE）等价于最小二乘法。引入参数的<strong class="keyword">高斯先验</strong>则得到最大后验估计（MAP），这等价于L2正则化（岭回归）。</li>
                        </ul>
                    </li>
                </ul>
            </details>

            <details>
                <summary>第10章: 主成分分析 (PCA) 用于降维</summary>
                <ul>
                    <li><strong>问题:</strong> 在保留数据最多信息（方差）的前提下，将高维数据投影到低维子空间。</li>
                    <li><strong>数学工具:</strong>
                        <ul>
                            <li><strong class="keyword">两个视角:</strong> 1) 寻找一个投影方向，使得数据投影后的<strong class="keyword">方差最大</strong>。2) 寻找一个低维子空间，使得数据点到其投影的<strong class="keyword">重构误差最小</strong>。这两个视角是等价的。</li>
                            <li><strong class="keyword">矩阵分解:</strong> PCA的解是数据<strong class="keyword">协方差矩阵</strong>的<strong class="keyword">特征向量</strong>。主成分是与最大特征值对应的特征向量。SVD提供了一种更数值稳定的计算方法。</li>
                        </ul>
                    </li>
                </ul>
            </details>

            <details>
                <summary>第11章: 高斯混合模型 (GMM) 用于密度估计</summary>
                <ul>
                    <li><strong>问题:</strong> 用概率分布来描述数据集的结构，特别是当数据包含多个簇（多峰分布）时。</li>
                    <li><strong>数学工具:</strong>
                        <ul>
                            <li><strong class="keyword">概率论:</strong> 模型是多个高斯分布的<strong class="keyword">加权和</strong>（混合）。</li>
                            <li><strong class="keyword">隐变量 (Latent Variable):</strong> 引入一个离散的隐变量，表示每个数据点属于哪个高斯分量。</li>
                            <li><strong class="keyword">优化:</strong> 由于直接最大化似然函数没有闭式解，使用<strong class="keyword">期望最大化 (EM) 算法</strong>进行迭代求解。EM算法在E步计算每个点属于各分量的“责任”（后验概率），在M步利用这些责任更新高斯分量的参数。</li>
                        </ul>
                    </li>
                </ul>
            </details>

            <details>
                <summary>第12章: 支持向量机 (SVM) 用于分类</summary>
                <ul>
                    <li><strong>问题:</strong> 找到一个超平面，以尽可能大的“间隔”将两类数据点分开。</li>
                    <li><strong>数学工具:</strong>
                        <ul>
                            <li><strong class="keyword">解析几何:</strong> 核心是<strong class="keyword">超平面</strong> <code>(⟨w, x⟩ + b = 0)</code> 和<strong class="keyword">间隔 (margin)</strong> 的几何概念。最大化间隔等价于最小化 <code>||w||²</code>。</li>
                            <li><strong class="keyword">约束优化:</strong> 这是一个带不等式约束的二次规划问题。通过<strong class="keyword">拉格朗日对偶</strong>，可以转化为一个更易于求解（且能引出核技巧）的对偶问题。</li>
                            <li><strong class="keyword">核技巧 (Kernel Trick):</strong> 通过核函数 <code>k(xᵢ, xⱼ) = ⟨φ(xᵢ), φ(xⱼ)⟩</code>，在<strong class="keyword">高维特征空间</strong>中隐式地计算内积，从而实现非线性分类，而无需显式地进行特征映射 <code>φ(·)</code>。</li>
                        </ul>
                    </li>
                </ul>
            </details>
        </details>
    </main>

<div class="attachments-panel" id="attachments-panel">
    <h3>原文</h3>
    <a href="book_math_for_ML_files/book.html" target="_blank">源链接</a>
    <h3>附件</h3>
        <div class="attachment-item">
            <h4>中文PDF <span class="file-size">(48.0M)</span></h4>
            <a href="book_math_for_ML_files/book.pdf" class="download-btn" download>下载</a>
        </div>
        <div class="attachment-item">
            <h4>中文epub <span class="file-size">(19.9M)</span></h4>
            <a href="book_math_for_ML_files/book.epub" class="download-btn" download>下载</a>
        </div>
</div>
<button class="float-toggle" id="float-toggle" title="打开附件面板">◁</button>
<script>
// 浮动框展开/收起功能
document.addEventListener('DOMContentLoaded', function() {
    const panel = document.getElementById('attachments-panel');
    const toggleBtn = document.getElementById('float-toggle');
    
    if (!panel || !toggleBtn) return;
    
    // 切换面板显示状态
    function togglePanel() {
        panel.classList.toggle('expanded');
        // 更新按钮图标
        const isExpanded = panel.classList.contains('expanded');
        toggleBtn.textContent = isExpanded ? '▷' : '◁';
        toggleBtn.title = isExpanded ? '关闭附件面板' : '打开附件面板';
        
        // 动态调整按钮位置
        if (isExpanded) {
            // 等待面板展开动画完成后调整按钮位置
            setTimeout(() => {
                const panelWidth = panel.offsetWidth;
                toggleBtn.style.right = (panelWidth + 30) + 'px';
            }, 300);
        } else {
            toggleBtn.style.right = '10px';
        }
    }
    
    // 点击切换按钮
    toggleBtn.addEventListener('click', function(e) {
        e.preventDefault();
        e.stopPropagation();
        togglePanel();
    });
    
    // 点击页面其他地方时收起面板
    document.addEventListener('click', function(e) {
        const isClickOnPanel = panel.contains(e.target);
        const isClickOnToggle = toggleBtn.contains(e.target);
        
        if (!isClickOnPanel && !isClickOnToggle) {
            panel.classList.remove('expanded');
            toggleBtn.textContent = '◁';
            toggleBtn.title = '打开附件面板';
            toggleBtn.style.right = '10px'; // 重置按钮位置
        }
    });
    
    // ESC键关闭面板
    document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape' && panel.classList.contains('expanded')) {
            panel.classList.remove('expanded');
            toggleBtn.textContent = '◁';
            toggleBtn.title = '打开附件面板';
            toggleBtn.style.right = '10px'; // 重置按钮位置
        }
    });
});
</script>
</body>
</html>